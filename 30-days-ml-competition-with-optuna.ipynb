{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/upamanyumukherjee/30-days-ml-competition-with-optuna?scriptVersionId=88621883\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"Welcome to the **[30 Days of ML competition](https://www.kaggle.com/c/30-days-of-ml/overview)**!  In this notebook, you'll learn how to make your first submission.\n\nBefore getting started, make your own editable copy of this notebook by clicking on the **Copy and Edit** button.\n\n# Step 1: Import helpful libraries\n\nWe begin by importing the libraries we'll need.  Some of them will be familiar from the **[Intro to Machine Learning](https://www.kaggle.com/learn/intro-to-machine-learning)** course and the **[Intermediate Machine Learning](https://www.kaggle.com/learn/intermediate-machine-learning)** course.","metadata":{}},{"cell_type":"code","source":"# Familiar imports\nimport numpy as np\nimport pandas as pd\n\n# For ordinal encoding categorical variables, splitting data\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.model_selection import train_test_split\n\n# For training random forest model\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error","metadata":{"execution":{"iopub.status.busy":"2021-09-23T11:17:31.07761Z","iopub.execute_input":"2021-09-23T11:17:31.07801Z","iopub.status.idle":"2021-09-23T11:17:32.054968Z","shell.execute_reply.started":"2021-09-23T11:17:31.077926Z","shell.execute_reply":"2021-09-23T11:17:32.054127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 2: Load the data\n\nNext, we'll load the training and test data.  \n\nWe set `index_col=0` in the code cell below to use the `id` column to index the DataFrame.  (*If you're not sure how this works, try temporarily removing `index_col=0` and see how it changes the result.*)","metadata":{}},{"cell_type":"code","source":"# Load the training data\ntrain = pd.read_csv(\"../input/30-days-of-ml/train.csv\", index_col=0)\ntest = pd.read_csv(\"../input/30-days-of-ml/test.csv\", index_col=0)\n\n# Preview the data\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-23T11:17:32.058053Z","iopub.execute_input":"2021-09-23T11:17:32.05834Z","iopub.status.idle":"2021-09-23T11:17:35.451363Z","shell.execute_reply.started":"2021-09-23T11:17:32.058312Z","shell.execute_reply":"2021-09-23T11:17:35.450536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The next code cell separates the target (which we assign to `y`) from the training features (which we assign to `features`).","metadata":{}},{"cell_type":"code","source":"# Separate target from features\ny = train['target']\nfeatures = train.drop(['target'], axis=1)\n\n# Preview features\nfeatures.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-23T11:17:35.453445Z","iopub.execute_input":"2021-09-23T11:17:35.453832Z","iopub.status.idle":"2021-09-23T11:17:35.507881Z","shell.execute_reply.started":"2021-09-23T11:17:35.453795Z","shell.execute_reply":"2021-09-23T11:17:35.506855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 3: Prepare the data\n\nNext, we'll need to handle the categorical columns (`cat0`, `cat1`, ... `cat9`).  \n\nIn the **[Categorical Variables lesson](https://www.kaggle.com/alexisbcook/categorical-variables)** in the Intermediate Machine Learning course, you learned several different ways to encode categorical variables in a dataset.  In this notebook, we'll use ordinal encoding and save our encoded features as new variables `X` and `X_test`.","metadata":{}},{"cell_type":"code","source":"# List of categorical columns\nobject_cols = [col for col in features.columns if 'cat' in col]\n\n# ordinal-encode categorical columns\nX = features.copy()\nX_test = test.copy()\nordinal_encoder = OrdinalEncoder()\nX[object_cols] = ordinal_encoder.fit_transform(features[object_cols])\nX_test[object_cols] = ordinal_encoder.transform(test[object_cols])\n\n# Preview the ordinal-encoded features\nX.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-23T11:17:35.509954Z","iopub.execute_input":"2021-09-23T11:17:35.510373Z","iopub.status.idle":"2021-09-23T11:17:39.423009Z","shell.execute_reply.started":"2021-09-23T11:17:35.510322Z","shell.execute_reply":"2021-09-23T11:17:39.422089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we break off a validation set from the training data.","metadata":{}},{"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=0)","metadata":{"execution":{"iopub.status.busy":"2021-09-23T11:17:39.424375Z","iopub.execute_input":"2021-09-23T11:17:39.424784Z","iopub.status.idle":"2021-09-23T11:17:39.539459Z","shell.execute_reply.started":"2021-09-23T11:17:39.424744Z","shell.execute_reply":"2021-09-23T11:17:39.538493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 4: Train a model\n\nNow that the data is prepared, the next step is to train a model.  \n\nIf you took the **[Intro to Machine Learning](https://www.kaggle.com/learn/intro-to-machine-learning)** courses, then you learned about **[Random Forests](https://www.kaggle.com/dansbecker/random-forests)**.  In the code cell below, we fit a random forest model to the data.","metadata":{}},{"cell_type":"code","source":"*Baseline","metadata":{"execution":{"iopub.status.busy":"2021-09-23T12:24:08.93339Z","iopub.execute_input":"2021-09-23T12:24:08.933749Z","iopub.status.idle":"2021-09-23T12:24:08.93966Z","shell.execute_reply.started":"2021-09-23T12:24:08.933718Z","shell.execute_reply":"2021-09-23T12:24:08.938433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Baseline**","metadata":{}},{"cell_type":"code","source":"# Define the model \nmodel = RandomForestRegressor(random_state=1)\n\n# Train the model (will take about 10 minutes to run)\nmodel.fit(X_train, y_train)\npreds_valid = model.predict(X_valid)\nprint(mean_squared_error(y_valid, preds_valid, squared=False))","metadata":{"execution":{"iopub.status.busy":"2021-09-23T11:17:39.540766Z","iopub.execute_input":"2021-09-23T11:17:39.541164Z","iopub.status.idle":"2021-09-23T11:26:53.654087Z","shell.execute_reply.started":"2021-09-23T11:17:39.541127Z","shell.execute_reply":"2021-09-23T11:26:53.653179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use the model to generate predictions\npredictions_raf = model.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-09-23T11:26:53.655319Z","iopub.execute_input":"2021-09-23T11:26:53.655652Z","iopub.status.idle":"2021-09-23T11:27:03.680542Z","shell.execute_reply.started":"2021-09-23T11:26:53.655617Z","shell.execute_reply":"2021-09-23T11:27:03.67958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Using Optuna to tune our models and then ensemble them.","metadata":{}},{"cell_type":"markdown","source":"### With Optuna we can do hyperparameter tuning. It is the problem of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a parameter whose value is used to control the learning process. By contrast, the values of other parameters (typically node weights) are learned.\n### In the following steps have demonstrated it in three models **LGBM, Xboost Classifier and Random Forest**.","metadata":{}},{"cell_type":"code","source":"def objective(trial, X, y, name='xgb'):\n        \n    params = {'max_depth':trial.suggest_int('max_depth', 5, 50),\n              'n_estimators':200000,\n              #'boosting':trial.suggest_categorical('boosting', ['gbdt', 'dart', 'goss']),\n              'subsample': trial.suggest_uniform('subsample', 0.2, 1.0),\n              'colsample_bytree':trial.suggest_uniform('colsample_bytree', 0.2, 1.0),\n              'learning_rate':trial.suggest_uniform('learning_rate', 0.007, 0.02),\n              'reg_lambda':trial.suggest_uniform('reg_lambda', 0.01, 50),\n              'reg_alpha':trial.suggest_uniform('reg_alpha', 0.01, 50),\n              'min_child_samples':trial.suggest_int('min_child_samples', 5, 100),\n              'num_leaves':trial.suggest_int('num_leaves', 10, 200),\n              'n_jobs' : -1,\n              'metric':'rmse',\n              'max_bin':trial.suggest_int('max_bin', 300, 1000),\n              'cat_smooth':trial.suggest_int('cat_smooth', 5, 100),\n              'cat_l2':trial.suggest_loguniform('cat_l2', 1e-3, 100)}\n\n    model = LGBMRegressor(**params)\n                  \n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0)\n    \n\n    model.fit(X_train, y_train, eval_set=[(X_val, y_val)],\n              eval_metric=['rmse'],\n              early_stopping_rounds=250, \n              categorical_feature=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n              #callbacks=[optuna.integration.LightGBMPruningCallback(trial, metric='rmse')],\n              verbose=0)\n\n    train_score = np.round(np.sqrt(mean_squared_error(y_train, model.predict(X_train))), 5)\n    test_score = np.round(np.sqrt(mean_squared_error(y_val, model.predict(X_val))), 5)\n                  \n    print(f'TRAIN RMSE : {train_score} || TEST RMSE : {test_score}')\n                  \n    return test_score","metadata":{"execution":{"iopub.status.busy":"2021-09-23T11:27:03.68196Z","iopub.execute_input":"2021-09-23T11:27:03.682503Z","iopub.status.idle":"2021-09-23T11:27:03.693978Z","shell.execute_reply.started":"2021-09-23T11:27:03.682456Z","shell.execute_reply":"2021-09-23T11:27:03.692943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from lightgbm import LGBMRegressor\n\nimport optuna\nfrom functools import partial","metadata":{"execution":{"iopub.status.busy":"2021-09-23T11:27:03.697283Z","iopub.execute_input":"2021-09-23T11:27:03.697721Z","iopub.status.idle":"2021-09-23T11:27:05.820202Z","shell.execute_reply.started":"2021-09-23T11:27:03.69769Z","shell.execute_reply":"2021-09-23T11:27:05.819361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimize = partial(objective, X=X_train, y=y_train)\n\nstudy_lgbm = optuna.create_study(direction='minimize')\n# study_lgbm.optimize(optimize, n_trials=300)\n\n# i have commented out the trials so as to cut short the notebook execution time.","metadata":{"execution":{"iopub.status.busy":"2021-09-23T11:27:05.822261Z","iopub.execute_input":"2021-09-23T11:27:05.822639Z","iopub.status.idle":"2021-09-23T11:27:05.832246Z","shell.execute_reply.started":"2021-09-23T11:27:05.822604Z","shell.execute_reply":"2021-09-23T11:27:05.831403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from xgboost import XGBRegressor\n\n# Define the model\ndef objective2(trial, X, y, name='xgb'):\n        \n    param = {\n            'tree_method':'gpu_hist',  # this parameter means using the GPU when training our model to speedup the training process\n            'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n            'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n            'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1.0]),\n            'subsample': trial.suggest_categorical('subsample', [0.4,0.5,0.6,0.7,0.8,1.0]),\n            'learning_rate': trial.suggest_categorical('learning_rate', [0.008,0.009,0.01,0.012,0.014,0.016,0.018, 0.02]),\n            'n_estimators': 4000,\n            'max_depth': trial.suggest_categorical('max_depth', [5,7,9,11,13,15,17,20]),\n            'random_state': trial.suggest_categorical('random_state', [24, 48,2020]),\n            'min_child_weight': trial.suggest_int('min_child_weight', 1, 300),\n        }\n\n#     model = LGBMRegressor(**params)\n    model = XGBRegressor(**param) # Your code here\n                  \n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0)\n    \n\n    model.fit(X_train, y_train, eval_set=[(X_val, y_val)],\n              eval_metric=['rmse'],\n              early_stopping_rounds=250, \n#               categorical_feature=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n              #callbacks=[optuna.integration.LightGBMPruningCallback(trial, metric='rmse')],\n              verbose=0)\n\n    train_score = np.round(np.sqrt(mean_squared_error(y_train, model.predict(X_train))), 5)\n    test_score = np.round(np.sqrt(mean_squared_error(y_val, model.predict(X_val))), 5)\n                  \n    print(f'TRAIN RMSE : {train_score} || TEST RMSE : {test_score}')\n                  \n    return test_score","metadata":{"execution":{"iopub.status.busy":"2021-09-23T11:27:05.833556Z","iopub.execute_input":"2021-09-23T11:27:05.834076Z","iopub.status.idle":"2021-09-23T11:27:05.90605Z","shell.execute_reply.started":"2021-09-23T11:27:05.83404Z","shell.execute_reply":"2021-09-23T11:27:05.905247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimize = partial(objective2, X=X_train, y=y_train)\n\nstudy_xgboost = optuna.create_study(direction='minimize')\n# study_xgboost.optimize(optimize, n_trials=300)\n\n# i have commented out the trials so as to cut short the notebook execution time.","metadata":{"execution":{"iopub.status.busy":"2021-09-23T11:27:05.907215Z","iopub.execute_input":"2021-09-23T11:27:05.907577Z","iopub.status.idle":"2021-09-23T11:27:05.914367Z","shell.execute_reply.started":"2021-09-23T11:27:05.90754Z","shell.execute_reply":"2021-09-23T11:27:05.913491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the model\ndef objective3(trial, X, y, name='xgb'):\n        \n    params = {\n            'n_estimators': trial.suggest_int('n_estimators', 50, 1000),\n            'max_depth': trial.suggest_int('max_depth', 4, 50),\n            'min_samples_split': trial.suggest_int('min_samples_split', 1, 150),\n            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 60),\n        }\n\n#     model = LGBMRegressor(**params)\n    model = RandomForestRegressor(**params)\n                  \n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0)\n    \n\n    model.fit(X_train, y_train,\n#               early_stopping_rounds=250, \n#               categorical_feature=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n              #callbacks=[optuna.integration.LightGBMPruningCallback(trial, metric='rmse')],\n             )\n\n    train_score = np.round(np.sqrt(mean_squared_error(y_train, model.predict(X_train))), 5)\n    test_score = np.round(np.sqrt(mean_squared_error(y_val, model.predict(X_val))), 5)\n                  \n    print(f'TRAIN RMSE : {train_score} || TEST RMSE : {test_score}')\n                  \n    return test_score","metadata":{"execution":{"iopub.status.busy":"2021-09-23T11:27:05.915953Z","iopub.execute_input":"2021-09-23T11:27:05.916401Z","iopub.status.idle":"2021-09-23T11:27:05.92698Z","shell.execute_reply.started":"2021-09-23T11:27:05.916366Z","shell.execute_reply":"2021-09-23T11:27:05.926149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimize = partial(objective3, X=X_train, y=y_train)\n\nstudy_rnforst = optuna.create_study(direction='minimize')\n# study_rnforst.optimize(optimize, n_trials=300)","metadata":{"execution":{"iopub.status.busy":"2021-09-23T11:27:05.929684Z","iopub.execute_input":"2021-09-23T11:27:05.929971Z","iopub.status.idle":"2021-09-23T11:27:05.939205Z","shell.execute_reply.started":"2021-09-23T11:27:05.929939Z","shell.execute_reply":"2021-09-23T11:27:05.938184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Save(X, y, name='lgb'):\n        \n    params = {'max_depth':31,\n              'n_estimators':200000,\n              #'boosting':trial.suggest_categorical('boosting', ['gbdt', 'dart', 'goss']),\n              'subsample': 0.3258744755198934,\n              'colsample_bytree':0.21016860689504144,\n              'learning_rate':0.01796483827009817,\n              'reg_lambda':18.9086111285175,\n              'reg_alpha':30.781820149384465,\n              'min_child_samples':44,\n              'num_leaves':200,\n              'n_jobs' : -1,\n              'metric':'rmse',\n              'max_bin':788,\n              'cat_smooth':34,\n              'cat_l2':10.706502107212572}\n\n    model = LGBMRegressor(**params)\n    k = 4\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0)\n    num_val_samples = len(X_train) // k\n    X_train=np.array(X_train)\n    y_train=np.array(y_train)\n    X_val=np.array(X_val)\n    y_val=np.array(y_val)\n    all_scores = []\n    for i in range(k):\n        print('processing fold #%d' % i)\n        X_val = X_train[i * num_val_samples: (i + 1) * num_val_samples] #taking data from a range of kth to kth +1 samples\n        y_val = y_train[i * num_val_samples: (i + 1) * num_val_samples]\n        partial_train_data = np.concatenate( \n            [X_train[:i * num_val_samples],\n             X_train[(i + 1) * num_val_samples:]],\n            axis=0)#taking data from a range of kth to kth +1 samples\n        partial_train_labels = np.concatenate(\n            [y_train[:i * num_val_samples],\n             y_train[(i + 1) * num_val_samples:]],\n            axis=0)\n    \n    model.fit(partial_train_data, partial_train_labels, eval_set=[(X_val, y_val)],\n              eval_metric=['rmse'],\n              early_stopping_rounds=250, \n              categorical_feature=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n              #callbacks=[optuna.integration.LightGBMPruningCallback(trial, metric='rmse')],\n              verbose=0)\n    preds_valid = model.predict(X_val)\n    y_val=np.array(y_val)\n    predictions = model.predict(X_test)\n                  \n    return predictions,preds_valid","metadata":{"execution":{"iopub.status.busy":"2021-09-23T11:27:05.940559Z","iopub.execute_input":"2021-09-23T11:27:05.941095Z","iopub.status.idle":"2021-09-23T11:27:05.954799Z","shell.execute_reply.started":"2021-09-23T11:27:05.941042Z","shell.execute_reply":"2021-09-23T11:27:05.953886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_lgb_valid=Save(X, y, name='lgb')[1]\npredictions_lgb_valid","metadata":{"execution":{"iopub.status.busy":"2021-09-23T11:27:05.956189Z","iopub.execute_input":"2021-09-23T11:27:05.956833Z","iopub.status.idle":"2021-09-23T11:29:25.770626Z","shell.execute_reply.started":"2021-09-23T11:27:05.956664Z","shell.execute_reply":"2021-09-23T11:29:25.769896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0)","metadata":{"execution":{"iopub.status.busy":"2021-09-23T11:35:46.831201Z","iopub.execute_input":"2021-09-23T11:35:46.831633Z","iopub.status.idle":"2021-09-23T11:35:46.914581Z","shell.execute_reply.started":"2021-09-23T11:35:46.831591Z","shell.execute_reply":"2021-09-23T11:35:46.91372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_score_lgb = np.round(np.sqrt(mean_squared_error(y_val, predictions_lgb_valid)), 5)\ntest_score_lgb","metadata":{"execution":{"iopub.status.busy":"2021-09-23T12:29:45.045129Z","iopub.execute_input":"2021-09-23T12:29:45.045457Z","iopub.status.idle":"2021-09-23T12:29:45.054641Z","shell.execute_reply.started":"2021-09-23T12:29:45.045428Z","shell.execute_reply":"2021-09-23T12:29:45.053522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot = []\nplotname = []","metadata":{"execution":{"iopub.status.busy":"2021-09-23T12:30:01.66499Z","iopub.execute_input":"2021-09-23T12:30:01.665509Z","iopub.status.idle":"2021-09-23T12:30:01.670131Z","shell.execute_reply.started":"2021-09-23T12:30:01.66545Z","shell.execute_reply":"2021-09-23T12:30:01.669183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot.append(test_score_lgb)\nplotname.append('test_score_lgb')","metadata":{"execution":{"iopub.status.busy":"2021-09-23T12:30:06.0787Z","iopub.execute_input":"2021-09-23T12:30:06.07902Z","iopub.status.idle":"2021-09-23T12:30:06.083862Z","shell.execute_reply.started":"2021-09-23T12:30:06.078986Z","shell.execute_reply":"2021-09-23T12:30:06.082835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the predictions to a CSV file\npredictions_lgb=Save(X, y, name='lgb')[0]\noutput_lgb = pd.DataFrame({'Id': X_test.index,\n                       'target': predictions_lgb})\noutput_lgb.to_csv('submission1.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-23T11:35:49.272118Z","iopub.execute_input":"2021-09-23T11:35:49.272441Z","iopub.status.idle":"2021-09-23T11:38:11.428808Z","shell.execute_reply.started":"2021-09-23T11:35:49.272413Z","shell.execute_reply":"2021-09-23T11:38:11.42789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Save(X, y, name='xgb'):\n        \n    params = {'lambda': 5.448403383226208,\n              'alpha': 0.004852988858499958,\n              'colsample_bytree': 0.4,\n              'subsample': 0.7,\n              'learning_rate': 0.018,\n              'max_depth': 15,\n              'random_state': 24,\n              'min_child_weight': 291\n             }\n    \n    model = XGBRegressor(**params) # Your code here\n    k = 4\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0)\n    num_val_samples = len(X_train) // k\n    X_train=np.array(X_train)\n    y_train=np.array(y_train)\n    X_val=np.array(X_val)\n    y_val=np.array(y_val)\n    all_scores = []\n    for i in range(k):\n        print('processing fold #%d' % i)\n        X_val = X_train[i * num_val_samples: (i + 1) * num_val_samples] #taking data from a range of kth to kth +1 samples\n        y_val = y_train[i * num_val_samples: (i + 1) * num_val_samples]\n        partial_train_data = np.concatenate( \n            [X_train[:i * num_val_samples],\n             X_train[(i + 1) * num_val_samples:]],\n            axis=0)#taking data from a range of kth to kth +1 samples\n        partial_train_labels = np.concatenate(\n            [y_train[:i * num_val_samples],\n             y_train[(i + 1) * num_val_samples:]],\n            axis=0)\n    \n    model.fit(X_train, y_train, eval_set=[(X_val, y_val)],\n              eval_metric=['rmse'],\n              early_stopping_rounds=250, \n#               categorical_feature=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n              #callbacks=[optuna.integration.LightGBMPruningCallback(trial, metric='rmse')],\n              verbose=0)\n    preds_valid = model.predict(X_val)\n    y_val=np.array(y_val)\n    predictions = model.predict(X_test)\n                  \n    return predictions,preds_valid","metadata":{"execution":{"iopub.status.busy":"2021-09-23T11:38:11.430305Z","iopub.execute_input":"2021-09-23T11:38:11.430651Z","iopub.status.idle":"2021-09-23T11:38:11.44237Z","shell.execute_reply.started":"2021-09-23T11:38:11.430618Z","shell.execute_reply":"2021-09-23T11:38:11.441286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_xgb_valid=Save(X, y, name='xgb')[1]\npredictions_xgb_valid","metadata":{"execution":{"iopub.status.busy":"2021-09-23T11:38:11.444773Z","iopub.execute_input":"2021-09-23T11:38:11.445343Z","iopub.status.idle":"2021-09-23T11:38:36.236024Z","shell.execute_reply.started":"2021-09-23T11:38:11.445302Z","shell.execute_reply":"2021-09-23T11:38:36.23516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_score_xgb = np.round(np.sqrt(mean_squared_error(y_val, predictions_xgb_valid)), 5)\ntest_score_xgb","metadata":{"execution":{"iopub.status.busy":"2021-09-23T12:29:02.786927Z","iopub.execute_input":"2021-09-23T12:29:02.787299Z","iopub.status.idle":"2021-09-23T12:29:02.795958Z","shell.execute_reply.started":"2021-09-23T12:29:02.787266Z","shell.execute_reply":"2021-09-23T12:29:02.794875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot.append(test_score_xgb)\nplotname.append('test_score_xgb')","metadata":{"execution":{"iopub.status.busy":"2021-09-23T12:30:14.386972Z","iopub.execute_input":"2021-09-23T12:30:14.387355Z","iopub.status.idle":"2021-09-23T12:30:14.390997Z","shell.execute_reply.started":"2021-09-23T12:30:14.387324Z","shell.execute_reply":"2021-09-23T12:30:14.390122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the predictions to a CSV file\npredictions_xgb=Save(X, y, name='xgb')[0]\noutput_xgb = pd.DataFrame({'Id': X_test.index,\n                       'target': predictions_xgb})\noutput_xgb.to_csv('submission2.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-23T11:38:36.252123Z","iopub.execute_input":"2021-09-23T11:38:36.252475Z","iopub.status.idle":"2021-09-23T11:39:00.721427Z","shell.execute_reply.started":"2021-09-23T11:38:36.252442Z","shell.execute_reply":"2021-09-23T11:39:00.720377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Save(X, y, name='rnf'):\n        \n    params = {'n_estimators': 54,\n              'max_depth': 29,\n              'min_samples_split': 121,\n              'min_samples_leaf': 38\n             }\n    model = RandomForestRegressor(**params)\n    k = 4\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0)\n    num_val_samples = len(X_train) // k\n    X_train=np.array(X_train)\n    y_train=np.array(y_train)\n    X_val=np.array(X_val)\n    y_val=np.array(y_val)\n    all_scores = []\n    for i in range(k):\n        print('processing fold #%d' % i)\n        X_val = X_train[i * num_val_samples: (i + 1) * num_val_samples] #taking data from a range of kth to kth +1 samples\n        y_val = y_train[i * num_val_samples: (i + 1) * num_val_samples]\n        partial_train_data = np.concatenate( \n            [X_train[:i * num_val_samples],\n             X_train[(i + 1) * num_val_samples:]],\n            axis=0)#taking data from a range of kth to kth +1 samples\n        partial_train_labels = np.concatenate(\n            [y_train[:i * num_val_samples],\n             y_train[(i + 1) * num_val_samples:]],\n            axis=0)\n    \n    model.fit(partial_train_data, partial_train_labels)\n    preds_valid = model.predict(X_val)\n    y_val=np.array(y_val)\n    predictions = model.predict(X_test)\n                  \n    return predictions,preds_valid","metadata":{"execution":{"iopub.status.busy":"2021-09-23T11:39:00.725724Z","iopub.execute_input":"2021-09-23T11:39:00.727883Z","iopub.status.idle":"2021-09-23T11:39:00.741828Z","shell.execute_reply.started":"2021-09-23T11:39:00.727844Z","shell.execute_reply":"2021-09-23T11:39:00.740996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_rnf_valid =Save(X, y, name='rnf')[1]\npredictions_rnf_valid","metadata":{"execution":{"iopub.status.busy":"2021-09-23T11:39:00.746775Z","iopub.execute_input":"2021-09-23T11:39:00.749359Z","iopub.status.idle":"2021-09-23T11:41:32.456393Z","shell.execute_reply.started":"2021-09-23T11:39:00.749323Z","shell.execute_reply":"2021-09-23T11:41:32.455528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_score_RNF = np.round(np.sqrt(mean_squared_error(y_val, predictions_rnf_valid)), 5)\ntest_score_RNF","metadata":{"execution":{"iopub.status.busy":"2021-09-23T11:41:32.458947Z","iopub.execute_input":"2021-09-23T11:41:32.459339Z","iopub.status.idle":"2021-09-23T11:41:32.46736Z","shell.execute_reply.started":"2021-09-23T11:41:32.459302Z","shell.execute_reply":"2021-09-23T11:41:32.466402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot.append(test_score_RNF)\nplotname.append('test_score_RNF')","metadata":{"execution":{"iopub.status.busy":"2021-09-23T12:30:20.892342Z","iopub.execute_input":"2021-09-23T12:30:20.892663Z","iopub.status.idle":"2021-09-23T12:30:20.896916Z","shell.execute_reply.started":"2021-09-23T12:30:20.892635Z","shell.execute_reply":"2021-09-23T12:30:20.895746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the predictions to a CSV file\npredictions_rnf=Save(X, y, name='rnf')[0]\noutput_rnf = pd.DataFrame({'Id': X_test.index,\n                       'target': predictions_rnf})\noutput_rnf.to_csv('submission3.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-23T11:41:32.468975Z","iopub.execute_input":"2021-09-23T11:41:32.469394Z","iopub.status.idle":"2021-09-23T11:44:02.879887Z","shell.execute_reply.started":"2021-09-23T11:41:32.469356Z","shell.execute_reply":"2021-09-23T11:44:02.879039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Ensemble","metadata":{}},{"cell_type":"code","source":"predictions_ensemble_valid=(predictions_lgb_valid+predictions_xgb_valid+predictions_rnf_valid)/3","metadata":{"execution":{"iopub.status.busy":"2021-09-23T11:44:02.881149Z","iopub.execute_input":"2021-09-23T11:44:02.881504Z","iopub.status.idle":"2021-09-23T11:44:02.885729Z","shell.execute_reply.started":"2021-09-23T11:44:02.881468Z","shell.execute_reply":"2021-09-23T11:44:02.88485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_ensemble = (predictions_lgb + predictions_xgb + predictions_rnf)/3","metadata":{"execution":{"iopub.status.busy":"2021-09-23T12:01:05.897604Z","iopub.execute_input":"2021-09-23T12:01:05.89798Z","iopub.status.idle":"2021-09-23T12:01:05.902705Z","shell.execute_reply.started":"2021-09-23T12:01:05.897943Z","shell.execute_reply":"2021-09-23T12:01:05.901856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_ensemble_valid","metadata":{"execution":{"iopub.status.busy":"2021-09-23T11:44:02.887281Z","iopub.execute_input":"2021-09-23T11:44:02.887829Z","iopub.status.idle":"2021-09-23T11:44:02.907161Z","shell.execute_reply.started":"2021-09-23T11:44:02.887793Z","shell.execute_reply":"2021-09-23T11:44:02.906069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_score_ensemble = np.round(np.sqrt(mean_squared_error(y_val, predictions_ensemble_valid)), 5)\ntest_score_ensemble\nplot.append(test_score_ensemble)\nplotname.append('test_score_ensemble')","metadata":{"execution":{"iopub.status.busy":"2021-09-23T11:44:02.908516Z","iopub.execute_input":"2021-09-23T11:44:02.909101Z","iopub.status.idle":"2021-09-23T11:44:02.919061Z","shell.execute_reply.started":"2021-09-23T11:44:02.909021Z","shell.execute_reply":"2021-09-23T11:44:02.917933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2021-09-23T12:48:45.977741Z","iopub.execute_input":"2021-09-23T12:48:45.978089Z","iopub.status.idle":"2021-09-23T12:48:45.982853Z","shell.execute_reply.started":"2021-09-23T12:48:45.978056Z","shell.execute_reply":"2021-09-23T12:48:45.98193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualization","metadata":{}},{"cell_type":"code","source":"plt.bar(plotname,plot)","metadata":{"execution":{"iopub.status.busy":"2021-09-23T12:49:16.395368Z","iopub.execute_input":"2021-09-23T12:49:16.3957Z","iopub.status.idle":"2021-09-23T12:49:16.524852Z","shell.execute_reply.started":"2021-09-23T12:49:16.39567Z","shell.execute_reply":"2021-09-23T12:49:16.524002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the code cell above, we set `squared=False` to get the root mean squared error (RMSE) on the validation data.\n\n# Step 5: Submit to the competition\n\nWe'll begin by using the trained model to generate predictions, which we'll save to a CSV file.","metadata":{}},{"cell_type":"code","source":"# Save the predictions to a CSV file\noutput_ensemble = pd.DataFrame({'Id': X_test.index,\n                       'target': predictions_ensemble})\noutput_ensemble.to_csv('submission_new.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-23T12:02:09.746581Z","iopub.execute_input":"2021-09-23T12:02:09.746927Z","iopub.status.idle":"2021-09-23T12:02:10.712806Z","shell.execute_reply.started":"2021-09-23T12:02:09.746896Z","shell.execute_reply":"2021-09-23T12:02:10.711852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Next up:\n1. Would add Model Blending with current models.\n2. Would add Targert encoding.\n3. Would add DNN and RNNs as well in the model blending.","metadata":{}}]}