{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/upamanyumukherjee/nlp-disastertweetsnotebook?scriptVersionId=88421637\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"<h1 >Steps for Analysing Tweets with NLP and additional Tools</h1>","metadata":{}},{"cell_type":"markdown","source":"<bold>Data Visualization</bold>\n    1. Chloropeth Map of our Dataset with the help of Tableau\n    2. Length of Tweets of Train and Test\n    3. Which Keyword is related to disaster?\n    4. Wordcloud of Tweets\n    5. Ngrams of Most frequently occuring words\n<bold>Data Preprocessing</bold>\n    1. Cleaning with Regex\n    2. Converting the text to TFIDF\n<bold>Train and Test Model</bold>\n    1. Use the Naive Bayes Algorithmn\n    2. Save the output","metadata":{}},{"cell_type":"markdown","source":"# **Additionally**\n<bold>Using Sentiment analysizing,Topic Modelling</bold>\n    To check dataset is not bias to any side","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntrain.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test= pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\ntest.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport nltk \nimport string\nimport re\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom collections import defaultdict\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Keywords used in Tweets**","metadata":{}},{"cell_type":"code","source":"train['keyword'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['location'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Visualisations","metadata":{}},{"cell_type":"code","source":"# Import Tableau Visualisation \nfrom IPython.display import IFrame\nIFrame('https://public.tableau.com/views/DisasterTweets_16128035695960/Sheet1?:language=en&:retry=yes&:display_count=y&:origin=viz_share_link', width=1000, height=925)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[train['keyword'] == 'accident'].head(5)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Length of Train Tweets Vs Test Tweet","metadata":{}},{"cell_type":"code","source":"tweetLengthTrain = train['text'].str.len()\ntweetLengthTest = test['text'].str.len()\n\nplt.hist(tweetLengthTrain,bins=20,label='Train_Tweet')\nplt.hist(tweetLengthTest,bins=20,label='Test_Tweet')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[\"keyword\"].fillna(\"no_keywords\",inplace = True)\ntest[\"keyword\"].fillna(\"no_keywords\",inplace = True)\ntrain[\"location\"].fillna(\"no_location\",inplace=True)\ntest[\"location\"].fillna(\"no_location\",inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Which Keyword is Related to a Disaster?","metadata":{}},{"cell_type":"code","source":"train[\"target_mean\"] = train.groupby(\"keyword\")[\"target\"].transform(\"mean\")\n\nplt.figure(figsize=(8,72))\n\nsns.countplot(y=train.sort_values(\"target\",ascending=False)[\"keyword\"],hue=\\\n              train.sort_values(\"target\",ascending=False)[\"target\"])\nplt.tick_params(axis=\"x\",labelsize=15)\nplt.tick_params(axis=\"y\",labelsize=15)\nplt.title(\"Target Distribution in Keywords\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df=train\ntest_df=test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combine = train.append(test,ignore_index=True)\nprint('Shape of new Dataset:',combine.shape)\ncombine.tail()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_tweet = combine[\"text\"]\ntext_tweet.tail()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df =  pd.DataFrame(combine[['text']])\ndf.head(7)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_punct(text):\n    text  = \"\".join([char for char in text if char not in string.punctuation]) #all punctuations \n    text = re.sub('[0-9]+', '', text) #all numbers\n    return text\n\ndf['Tweet_punct'] = df['text'].apply(lambda x: remove_punct(x))\ndf.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"Tweet_punct\"].drop_duplicates(inplace = True)\ndf[\"Tweet_punct\"].shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combine[\"Tweet_punct\"] = df['Tweet_punct']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combine.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#important libraries for preprocessing using NLTK\nimport nltk\nfrom nltk import word_tokenize, FreqDist\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk.stem.porter import * ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = combine[combine['Tweet_punct'].notna()]\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Tokenize the tweets\n\ndef tokenization(text):\n    text = re.split('\\W+', text)\n    return text\n\ndf['Tweet_tokenized'] = df['Tweet_punct'].apply(lambda x: tokenization(x.lower()))\ndf.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Stemming the Tokenized Tweets","metadata":{}},{"cell_type":"markdown","source":"**Stemming algorithms work by cutting off the end or the beginning of the word, taking into account a list of common prefixes and suffixes that can be found in an inflected word. This indiscriminate cutting can be successful in some occasions, but not always, and that is why we affirm that this approach presents some limitations.**\n[Further Content](http://https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)","metadata":{}},{"cell_type":"code","source":"stemmer = PorterStemmer()\ndf['Tweet_tokenized'] = df['Tweet_tokenized'].apply(lambda x : [stemmer.stem(i) for i in x]  )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Joining the tokenized tweets\n\nfor i in range(len(df['Tweet_tokenized'])):\n    df['Tweet_tokenized'][i] = ' '.join(df['Tweet_tokenized'][i])    \ndf['cleanedText'] = df['Tweet_tokenized']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating word Cloud for all Words in all tweets\nfrom wordcloud import WordCloud\nallWords = ' '.join([text for text in df['cleanedText']])\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(allWords)\nplt.figure(figsize=(10, 10)) \nplt.imshow(wordcloud, interpolation=\"bilinear\") \nplt.axis('off') \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Sentiment Analysis**\nSentiment analysis is contextual mining of text which identifies and extracts subjective information in source material, and helping a business to understand the social sentiment of their brand, product or service while monitoring online conversations. \n[Sentiment Analysis Further Content](http://https://towardsdatascience.com/sentiment-analysis-concept-analysis-and-applications-6c94d6f58c17)","metadata":{}},{"cell_type":"code","source":"from textblob import TextBlob\n\ndef sentiment_polarity(text):\n    tweet_text = TextBlob(str(text))\n    sentiment_value = tweet_text.sentiment.polarity\n    return sentiment_value\n\ndf['Tweet_Polarity'] = df['cleanedText'].apply(lambda x: sentiment_polarity(x.lower()))\ndf.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sentiment_analysis(value):\n    sentiment=\"\"\n    if(value<0.0):\n        sentiment = \"negative\"\n    elif(value>0.0):\n        sentiment = \"positive\"\n    else:\n        sentiment = \"neutral\"\n    return sentiment\n\ndf['Tweet_Sentiments'] = df['Tweet_Polarity'].apply(lambda x: sentiment_analysis(float(x)))\ndf[[\"Tweet_punct\",\"Tweet_Polarity\"]].head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nax = sns.countplot(x=\"Tweet_Sentiments\", data=df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is done to check if the overall distribution of data was neutral. It is important as we are checking which tweet is a disaster so we have to a dataset which is not bias towards either side.","metadata":{}},{"cell_type":"markdown","source":"# **TF-IDF**","metadata":{}},{"cell_type":"markdown","source":"Term Frequency, which measures how frequently a term occurs in a document. Inverse Document Frequency, which measures how important a term is with respect to the document. TFIDF gives us a array which shows how frequeeently important occurs while taking into consideration conditions like Long Tail.Which can be managed by taking Log of the matrix.\n[Further content](http://https://towardsdatascience.com/natural-language-processing-feature-engineering-using-tf-idf-e8b9d00e7e76)","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer  \ntfidfconverter = TfidfVectorizer(max_features=2000, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))  \nX = tfidfconverter.fit_transform(df['cleanedText']).toarray()\nX","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenization(text):\n    text = re.split('\\W+', text)\n    return text\n\ndf['Tweet_tokenized'] = df['cleanedText'].apply(lambda x: tokenization(x.lower()))\ndf.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Topic Modeling-LDA**\nBy Topic Modelling we can check that which words where said highest and make sense of it by finding few common topics by the means ofprobabality","metadata":{}},{"cell_type":"code","source":"from gensim.corpora import Dictionary\n\n#create dictionary\ntext_dict = Dictionary(df.Tweet_tokenized)\n\n#view integer mappings\ntext_dict.token2id","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweets_bow = [text_dict.doc2bow(tweet) for tweet in df['Tweet_tokenized']]\ntweets_bow","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from gensim.models.ldamodel import LdaModel\n\nk = 10\ntweets_lda = LdaModel(tweets_bow,\n                      num_topics = k,\n                      id2word = text_dict,\n                      random_state = 1,\n                      passes=10)\n\ntweets_lda.show_topics()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Text Grouping by Regex**\nA RegEx, or Regular Expression, is a sequence of characters that forms a search pattern.\nRegEx can be used to check if a string contains the specified search pattern.\n[Further Content](http://https://www.w3schools.com/python/python_regex.asp)","metadata":{}},{"cell_type":"code","source":"# word_count\ntrain_df[\"word_count\"] = train_df[\"text\"].map(lambda x: len(str(x).split()))\ntest_df[\"word_count\"] = test_df[\"text\"].map(lambda x: len(str(x).split()))\n\n# unique_word_count \ntrain_df[\"unique_word_count\"] = train_df[\"text\"].map(lambda x:len(set(str(x).split())))\ntest_df[\"unique_word_count\"] = test_df[\"text\"].map(lambda x:len(set(str(x).split())))\n\n# stop_word_count\ntrain_df[\"stop_word_count\"] = train_df[\"text\"].map(lambda x: len([elt for elt in str(x).lower().split() \\\n                                                                  if elt in stopwords.words(\"english\")]))\ntest_df[\"stop_word_count\"] = test_df[\"text\"].map(lambda x:len([elt for elt in str(x).lower().split()\\\n                                                              if elt in stopwords.words(\"english\")]))\n# url_count \ntrain_df[\"url_count\"] = train_df[\"text\"].map(lambda x : len([w for w in str(x).lower().split()\\\n                                                            if 'http' or 'https' in w]))\ntest_df[\"url_count\"] = test_df[\"text\"].map(lambda x : len([w for w in str(x).lower().split()\\\n                                                          if \"http\" or \"https\" in w ]))\n# mean_word_length\ntrain_df[\"mean_word_length\"] = train_df[\"text\"].map(lambda x : np.mean([len(word) for word in x.split()]))\ntest_df[\"mean_word_length\"] = test_df[\"text\"].map(lambda x : np.mean([len(word) for word in x.split()]))\n\n#char_count \ntrain_df[\"char_count\"] = train_df[\"text\"].map(lambda x:len(str(x)))\ntest_df[\"char_count\"] = test_df[\"text\"].map(lambda x:len(str(x)))\n\n#punctuation_count\ntrain_df[\"punctuation_count\"] = train_df[\"text\"].map(lambda x: len([elt for elt in str(x) if elt in string.punctuation]))\ntest_df[\"punctuation_count\"] = test_df[\"text\"].map(lambda x:len([elt for elt in str(x) if elt in string.punctuation]))\n#hashtag_count\ntrain_df[\"hashtag_count\"] = train_df[\"text\"].apply(lambda x:len([c for c in str(x) if c==\"#\"]))\ntest_df[\"hashtag_count\"] = test_df[\"text\"].apply(lambda x:len([c for c in str(x) if c==\"#\"]))\n\n#mention_count\ntrain_df[\"mention_count\"] = train_df[\"text\"].map(lambda x: len([c for c in str(x) if c==\"@\"]))\ntest_df[\"mention_count\"] = test_df[\"text\"].map(lambda x: len([c for c in str(x) if c==\"@\"]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Plotting various Key Features**","metadata":{}},{"cell_type":"code","source":"# Heureunder, we will explore the distribution of each mega-feature per target and per dataset\n# (train & test)\nMETA_FEATURES = [\"word_count\",\"unique_word_count\",\"stop_word_count\",\"url_count\",\\\n                 \"mean_word_length\",\"char_count\",\"punctuation_count\",\"hashtag_count\",\"mention_count\"]\nfig,ax = plt.subplots(nrows=len(META_FEATURES),ncols=2,figsize=(20,50),dpi=100)\nmask = train_df[\"target\"]==1\nfor i,feature in enumerate(META_FEATURES):\n\n    \n   sns.distplot(train_df[mask][feature],ax=ax[i,0],label=\"Disaster\",kde=False)\n   sns.distplot(train_df[~mask][feature],ax=ax[i,0],label=\"Not Disaster\",kde=False)\n   ax[i,0].set_title(\"{} target distribution in trainning dataset\".format(feature),fontsize=13)\n \n   sns.distplot(train_df[feature],ax=ax[i,1],label=\"Train Dataset\",kde=False)\n   sns.distplot(test_df[feature],ax=ax[i,1],label=\"Test Dataset\",kde=False)\n   ax[i,1].set_title(\"{} training and test dataset distributions \".format(feature),fontsize=13)\n   for j in range(2):\n        ax[i,j].set_xlabel(\" \")\n        ax[i,j].tick_params(axis=\"x\",labelsize=13)\n        ax[i,j].tick_params(axis=\"y\",labelsize=13)\n        ax[i,j].legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Checking for Data imbalance**\nIt is important factor for determining whether we can mine data that can gives us answers under all conditions. In this case it is a balanced dataset[Further Content](http://https://www.analyticsvidhya.com/blog/2020/07/10-techniques-to-deal-with-class-imbalance-in-machine-learning/)","metadata":{}},{"cell_type":"code","source":"fig,ax = plt.subplots(1,2,figsize=(20,6))\n\ntrain_df.groupby(\"target\").count()[\"id\"].plot(kind=\"pie\",labels=[\"Not Disaster\",\"Disaster\"],\\\n                                              autopct=\"%1.1f pourcents\",ax=ax[0])\nsns.countplot(x=train_df[\"target\"],hue=train_df[\"target\"],ax=ax[1])\nax[1].set_xticklabels([\"Non Disaster\",\"Disaster\"])\nax[0].tick_params(axis=\"x\",labelsize=15)\nax[0].tick_params(axis=\"y\",labelsize=15)\nax[0].set_ylabel(\"\")\nax[1].tick_params(axis=\"x\",labelsize=15)\nax[1].tick_params(axis=\"y\",labelsize=15)\nax[0].set_title(\"Target distribution in training set\",fontsize=13)\nax[1].set_title(\"Target count in training set\",fontsize=13)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Creating high frequent grouped words in form of Ngrams**\nSimply put, an n-gram is a sequence of n words where n is a discrete number that can range from 1 to infinity!\n[Further Content](http://https://towardsdatascience.com/from-dataframe-to-n-grams-e34e29df3460)","metadata":{}},{"cell_type":"code","source":"# create the function which will be able to generate n_grams for each row of dataset.\ndef gen_n_grams(text,n_grams=1):\n    \"\"\" This function allow to extract the n_gram in the introduced text.\n    \n      @param text(str): the text that the function, will use to extract features (n_grams).\n      @param n_grams(int): the length of n_gram, that we will use.\n      @return ngrams(list): list of the ngrams in the intriduced text.\n    \"\"\"\n    tokens = [token for token in str(text).lower().split() if token not in stopwords.words(\"english\")]\n    ngrams = zip(*[tokens[i:] for i in range(n_grams)])\n    \n    return [\" \".join(gram) for gram in ngrams]\n\n# create the function which will be able to generate dataframe of n_gram features for disaster\n# and non disaster tweets.\ndef gen_df_ngrams(n_grams=1):\n    \"\"\" This function, allow to generate dataframes for n_grams in disaster tweets and non \n        disaster tweet\n    \"\"\"\n    mask = train_df[\"target\"]==1\n    disaster_unigrams = defaultdict(int)\n    non_disaster_unigrams = defaultdict(int)\n    \n    for tweet in train_df.loc[mask,\"text\"].values:\n        for gram in gen_n_grams(tweet,n_grams=n_grams):\n            disaster_unigrams[gram] +=1\n    for tweet in train_df.loc[~mask,\"text\"].values:\n        for gram in gen_n_grams(tweet,n_grams=n_grams):\n            non_disaster_unigrams[gram] +=1\n    df_disaster_n_grams = pd.DataFrame(sorted(disaster_unigrams.items(),reverse=True,key=\\\n                                              lambda item:item[1]))\n    df_non_disaster_n_grams = pd.DataFrame(sorted(non_disaster_unigrams.items(),reverse=True,key=\\\n                                                  lambda item:item[1]))\n    return df_disaster_n_grams,df_non_disaster_n_grams\n\n# Define function, which allow to plot the N most occured n_gram in disaster tweet \n# and non disaster tweet.\n\ndef plot_ngrams(df_disaster_n_grams,df_non_disaster_unigrams,N=100,n_grams=1):\n    \"\"\"This function,allow to plot the top most n_grams in disaster tweet and non disaser tweet.\n    \"\"\"\n    fig,ax = plt.subplots(1,2,figsize=(18,50))\n    sns.barplot(y=df_disaster_n_grams[0].values[:N],x=df_disaster_n_grams[1].values[:N],ax=ax[0],\\\n               color=\"red\")\n    for i in range(2):\n        ax[i].tick_params(axis=\"x\",labelsize=15)\n        ax[i].tick_params(axis=\"y\",labelsize=15)\n        ax[i].set_xlabel(\"Occurences\")\n        ax[i].spines[\"right\"].set_visible(False)\n    sns.barplot(y=df_non_disaster_unigrams[0].values[:N],x=df_non_disaster_unigrams[1].values[:N],\\\n               ax=ax[1],color=\"green\")\n    ax[0].set_title(\"Top most {} {}_grams for disaster tweets\".format(N,n_grams),size=15)\n    ax[1].set_title(\"Top most {} {}_grams for non disaster tweets\".format(N,n_grams),size=15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_disaster_unigrams,df_non_disaster_unigrams = gen_df_ngrams()\nplot_ngrams(df_disaster_unigrams,df_non_disaster_unigrams)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_disaster_unigrams,df_non_disaster_unigrams = gen_df_ngrams(n_grams=2)\nplot_ngrams(df_disaster_unigrams,df_non_disaster_unigrams,n_grams=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# extract the most 100 bigrams per target (Disaster and not Disaster) when n_grams=3\ndf_disaster_unigrams,df_non_disaster_unigrams = gen_df_ngrams(n_grams=3)\nplot_ngrams(df_disaster_unigrams,df_non_disaster_unigrams,n_grams=3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Data preprocessing for Modeling**","metadata":{}},{"cell_type":"code","source":"def process_text(text):\n    \"\"\"\n    Removes punctuations(if any), stopwords and returns a list words\n    \"\"\"\n    rm_pun = [char for char in text if char not in string.punctuation]\n    rm_pun = ''.join(rm_pun)\n    \n    return [word for word in rm_pun.split() if word.lower() not in stopwords.words('english')]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv = CountVectorizer(analyzer=process_text).fit(train_df['text'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data=train_df\ntest_data=test_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(cv.vocabulary_))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Sample Data for TFIDF**","metadata":{}},{"cell_type":"code","source":"text10 = train_data['text'][9]\ntext10","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv10 = cv.transform([text10])\nprint(cv10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(cv.get_feature_names()[5375])\nprint(cv.get_feature_names()[11579])\nprint(cv.get_feature_names()[11856])\nprint(cv.get_feature_names()[13190])\nprint(cv.get_feature_names()[25334])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv10.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data_cv = cv.transform(train_data['text'])\nprint(train_data_cv.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfTransformer\n\ntfidf = TfidfTransformer().fit(train_data_cv)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfidf10 = tfidf.transform(cv10)\nprint(tfidf10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data_tfidf = tfidf.transform(train_data_cv)\nprint(train_data_tfidf.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Naive Bayes Classifier** \nUnder the assumption that each data point is independent.It is a probalistic modelling Techinique in which probability of a tuple of a condition (Y|x) is the maximum of the two products of all the independent variables of either Yes or no condition.[Further Reading](http://https://machinelearningmastery.com/naive-bayes-classifier-scratch-python/)","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\ntarget_model = MultinomialNB().fit(train_data_tfidf, train_data['target'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('predicted:', target_model.predict(tfidf10)[0])\nprint('expected:', train_data.target[9])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Predictions for test data**\n","metadata":{}},{"cell_type":"code","source":"test_data_cv = cv.transform(test_data['text'])\n\ntest_data_tfidf = tfidf.transform(test_data_cv)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission = pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission['target'] = target_model.predict(test_data_tfidf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission.to_csv('/kaggle/working/submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}