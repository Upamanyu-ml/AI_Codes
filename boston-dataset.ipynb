{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/upamanyumukherjee/boston-dataset?scriptVersionId=88622001\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"# <font color=darkgreen> Building a Deep Neural Network for the Boston Housing Price dataset </font>\n\nThis dataset is relatively smaller than the previous datasets with only 506 data points (303 for training and 102 for testing,101 for validation).\nEach data point has a set of 13 features.","metadata":{"id":"dg-FEu4Ppc0b"}},{"cell_type":"markdown","source":"**Using all types of Sequential Neural Networks to see which model works best with this model dataset.**","metadata":{}},{"cell_type":"code","source":"import os\nimport sys\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.keras as keras\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"id":"Rov62fNxpc1k","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.datasets import boston_housing\n(train_data, train_labels), (test_data, test_labels) = boston_housing.load_data()","metadata":{"id":"q45muv6npc1r","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_data.shape)\nprint(test_data.shape)","metadata":{"id":"ih2FA4qnpc1x","outputId":"84991969-b789-470d-ba75-dfb6b922f6fe","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Data Preprocess**","metadata":{}},{"cell_type":"code","source":"# Normalize the data by subtracting the mean from each data point and\n# dividing by the standard deviation of the data\n\nmean = train_data.mean(axis=0) # since we want the mean for each feature column\nprint('Mean =', mean)\ntrain_data -= mean\n\nstd_dev = train_data.std(axis=0)\nprint('Std Dev = ', std_dev)\ntrain_data /= std_dev\n\n# Likewise prepare the test data (pre-processing)\ntest_data -= mean\ntest_data /= std_dev","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Holdout validation**\n# Creating a validation dataset from the train dataset thus forming the Holdout validation as we already have a test to evaluate on","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n#split dataset into train and test data\ntrain_data, val_data, train_labels, val_labels = train_test_split(train_data, train_labels, test_size=0.25, random_state=100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_labels.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_labels.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_labels.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# View the training labels\n# Prices are in 10,000s\nprint(train_labels[1:5])","metadata":{"id":"EMzVFz1fpc1z","outputId":"04c99211-4b92-47a4-c04e-3bb1f98a0866","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**We havetaken in Boston housing price 4 Dense [32, 32, 64, 1 ] relu in the first 3 layers and no activation in the final layer rmsprop lr=0.0001**","metadata":{}},{"cell_type":"code","source":"model = keras.Sequential([ \n        tf.keras.layers.Dense(32, activation='relu',input_shape=(404,13)),\n        tf.keras.layers.Dense(32, activation='relu'),\n        tf.keras.layers.Dense(64, activation='relu'),\n        tf.keras.layers.Dense(1)\n    ])\noptimizer=tf.keras.optimizers.RMSprop(lr=0.0001)\nmodel.compile(optimizer= optimizer, loss='mse', metrics=['mae']) # loss='mse' and metrics='mae'","metadata":{"id":"NarAvw-wpc15","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**To choose the appropriate epoch no I choose Early stopping to avoid overfitting**","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping\nearly_stop = EarlyStopping(monitor='val_loss', mode='min', patience=1,restore_best_weights=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history=model.fit(train_data, train_labels, epochs=200, batch_size=16,validation_data=(val_data,val_labels),callbacks=[early_stop])","metadata":{"id":"QqMtGcHlpc17","outputId":"214647e5-ee3a-4377-f6f8-6d366062a32d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"[mse, mae] = model.evaluate(test_data, test_labels) ","metadata":{"id":"3LO5e-Alpc1-","outputId":"556b2dd9-a7be-4ec6-f9d8-7eb818fadb59","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ypred = model.predict(test_data)\nprint('Actual Price = {} and Predicted Price = {}'.format(test_labels[1], ypred[1]))","metadata":{"id":"EZGLNbLipc2A","outputId":"36d2d14d-4d05-4266-b66b-f522d823f6f8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let us plot the loss and accuracy curves\nhistory_dict = history.history\nloss_value = history_dict['loss']\nval_loss_value = history_dict['val_loss']\nmae = history_dict['mae']\nval_mae = history_dict['val_mae']\nepochs = range(1, len(loss_value) + 1)\nplt.plot(epochs, loss_value, 'b', label='Training Loss')\nplt.plot(epochs, val_loss_value, 'r', label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\nplt.figure()\n\nplt.plot(epochs, mae, 'b', label='Training MAE')\nplt.plot(epochs, val_mae, 'r', label='Validation MAE')\nplt.title('Training and Validation MAE')\nplt.xlabel('Epochs')\nplt.ylabel('MAE')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**There is no significant overfitting or underfitting in this dataset based on the plots**","metadata":{}},{"cell_type":"markdown","source":"### Using K-fold validation","metadata":{"id":"xLYBlOvrpc2B"}},{"cell_type":"code","source":"# We create a fucntion to make it easy for multiple calls\ndef build_model():   \n    model = keras.Sequential([ \n        tf.keras.layers.Dense(32, activation='relu',input_shape=(404,13)),\n        tf.keras.layers.Dense(32, activation='relu'),\n        tf.keras.layers.Dense(64, activation='relu'),\n        tf.keras.layers.Dense(1)\n    ])\n    optimizer=tf.keras.optimizers.RMSprop(lr=0.0001)\n    model.compile(optimizer= optimizer, loss='mse', metrics=['mae']) # observe the loss and metrics\n    return model","metadata":{"id":"cvE3Mgsxpc2D","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let us visit K-fold validation\nk = 4\nnum_val_samples = len(train_data) // k\nnum_epochs = 100\nall_scores = []\nfor i in range(k):\n    print('processing fold #%d' % i)\n    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples] #taking data from a range of kth to kth +1 samples\n    val_labels = train_labels[i * num_val_samples: (i + 1) * num_val_samples]\n    partial_train_data = np.concatenate( \n        [train_data[:i * num_val_samples],\n         train_data[(i + 1) * num_val_samples:]],\n        axis=0)#taking data from a range of kth to kth +1 samples\n    partial_train_labels = np.concatenate(\n        [train_labels[:i * num_val_samples],\n         train_labels[(i + 1) * num_val_samples:]],\n        axis=0)\n    \nmodel = build_model() \nmodel.fit(partial_train_data, partial_train_labels, \n          epochs=num_epochs, batch_size=1, verbose=0)\nval_mse, val_mae = model.evaluate(val_data, val_labels, verbose=0) \nall_scores.append(val_mae)","metadata":{"id":"uNL2mDBIpc2F","outputId":"32a833b1-e6fa-4b2d-d018-67195094d552","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(all_scores)\nprint(np.mean(all_scores))","metadata":{"id":"au2ZYfZypc2I","outputId":"94407d96-5c8d-4f9b-b933-70ee4780f042","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping\nearly_stop = EarlyStopping(monitor='val_loss', mode='min', patience=1,restore_best_weights=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = build_model() \nhistory=model.fit(train_data, train_labels, epochs=280, batch_size=16,validation_data=(val_data,val_labels),callbacks=[early_stop])","metadata":{"id":"ocReoNQcpc2J","outputId":"0525d78c-3dce-47d9-9686-f59a74ed9dee","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_mse_score, test_mae_score = model.evaluate(test_data, test_labels)\nprint(test_mse_score, test_mae_score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ypred = model.predict(test_data)\nprint('Actual Price = {} and Predicted Price = {}'.format(test_labels[1], ypred[1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_dict","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let us plot the loss and accuracy curves\nhistory_dict = history.history\nloss_value = history_dict['loss']\nval_loss_value = history_dict['val_loss']\nmae = history_dict['mae']\nval_mae = history_dict['val_mae']\nepochs = range(1, len(loss_value) + 1)\nplt.plot(epochs, loss_value, 'b', label='Training Loss')\nplt.plot(epochs, val_loss_value, 'r', label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\nplt.figure()\n\nplt.plot(epochs, mae, 'b', label='Training Accuracy')\nplt.plot(epochs, val_mae, 'r', label='Validation Accuracy')\nplt.title('Training and Validation MAE')\nplt.xlabel('Epochs')\nplt.ylabel('MAE')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **DNN-BatchNormalization**[](http://)","metadata":{}},{"cell_type":"code","source":"model = keras.Sequential([ \n        tf.keras.layers.Dense(32, activation='relu',input_shape=(13,)),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(32, activation='relu'),\n        tf.keras.layers.Dense(64, activation='relu'),\n        tf.keras.layers.Dense(1)\n    ])\noptimizer=tf.keras.optimizers.SGD(lr=0.0001)\nmodel.compile(optimizer= optimizer, loss='mse', metrics=['mae']) # loss='mse' and metrics='mae'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history=model.fit(train_data, train_labels, epochs=180, batch_size=16,validation_data=(val_data,val_labels),callbacks=[early_stop])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_mse_score, test_mae_score = model.evaluate(test_data, test_labels)\nprint(test_mse_score, test_mae_score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ypred = model.predict(test_data)\nprint('Actual Price = {} and Predicted Price = {}'.format(test_labels[3], ypred[3]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let us plot the loss and accuracy curves\nhistory_dict = history.history\nloss_value = history_dict['loss']\nval_loss_value = history_dict['val_loss']\nmae = history_dict['mae']\nval_mae = history_dict['val_mae']\nepochs = range(1, len(loss_value) + 1)\nplt.plot(epochs, loss_value, 'b', label='Training Loss')\nplt.plot(epochs, val_loss_value, 'r', label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\nplt.figure()\n\nplt.plot(epochs, mae, 'b', label='Training Accuracy')\nplt.plot(epochs, val_mae, 'r', label='Validation Accuracy')\nplt.title('Training and Validation MAE')\nplt.xlabel('Epochs')\nplt.ylabel('MAE')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **GRU**","metadata":{}},{"cell_type":"code","source":"train_data_GRU = np.reshape(train_data, (train_data.shape[0],train_data.shape[1],1))\nmodel =  keras.Sequential([ \n        tf.keras.layers.GRU(16, return_sequences = True, activation='relu',input_shape=(train_data_GRU.shape[1],1)),\n        tf.keras.layers.GRU(16, return_sequences = True, activation='relu'),\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(1)\n    ])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer=tf.keras.optimizers.Adam(lr=0.0001)\nmodel.compile(optimizer=optimizer, loss='mse', metrics=['mae']) # observe the loss and metrics","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_data_GRU = np.reshape(val_data, (val_data.shape[0],val_data.shape[1],1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history=model.fit(train_data_GRU, train_labels, epochs=130, batch_size=16,validation_data=(val_data_GRU, val_labels),callbacks=[early_stop])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data_GRU = np.reshape(test_data, (test_data.shape[0],test_data.shape[1],1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"[mse, mae] = model.evaluate(test_data_GRU, test_labels) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ypred = model.predict(test_data_GRU)\nprint('Actual Price = {} and Predicted Price = {}'.format(test_labels[1], ypred[1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let us plot the loss and accuracy curves\nhistory_dict = history.history\nloss_value = history_dict['loss']\nval_loss_value = history_dict['val_loss']\nmae = history_dict['mae']\nval_mae = history_dict['val_mae']\nepochs = range(1, len(loss_value) + 1)\nplt.plot(epochs, loss_value, 'b', label='Training Loss')\nplt.plot(epochs, val_loss_value, 'r', label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\nplt.figure()\n\nplt.plot(epochs, mae, 'b', label='Training MAE')\nplt.plot(epochs, val_mae, 'r', label='Validation MAE')\nplt.title('Training and Validation MAE')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **CONV1D**","metadata":{}},{"cell_type":"code","source":"train_data_Conv1d = np.reshape(train_data, (train_data.shape[0],train_data.shape[1],1))#This is to reshape the x input data. We'll create one-dimensional vectors from each row of x input data.\nmodel =  keras.Sequential([ \n        tf.keras.layers.Conv1D(64, 1, activation='relu',input_shape=(train_data_Conv1d.shape[1],1)),\n        tf.keras.layers.Conv1D(64, 1, activation='relu'),\n        tf.keras.layers.MaxPooling1D(),\n        tf.keras.layers.Dropout(.5),\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(1)\n    ])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer=tf.keras.optimizers.SGD(lr=0.0001)\nmodel.compile(optimizer=optimizer, loss='mse', metrics=['mae']) # observe the loss and metrics","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_data_Con1D = np.reshape(val_data, (val_data.shape[0],val_data.shape[1],1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history=model.fit(train_data_Conv1d, train_labels, epochs=130, batch_size=16,validation_data=(val_data_Con1D, val_labels),callbacks=[early_stop])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data_Conv1D = np.reshape(test_data, (test_data.shape[0],test_data.shape[1],1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"[mse, mae] = model.evaluate(test_data_Conv1D, test_labels) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ypred = model.predict(test_data_Conv1D)\nprint('Actual Price = {} and Predicted Price = {}'.format(test_labels[1], ypred[1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let us plot the loss and accuracy curves\nhistory_dict = history.history\nloss_value = history_dict['loss']\nval_loss_value = history_dict['val_loss']\nmae = history_dict['mae']\nval_mae = history_dict['val_mae']\nepochs = range(1, len(loss_value) + 1)\nplt.plot(epochs, loss_value, 'b', label='Training Loss')\nplt.plot(epochs, val_loss_value, 'r', label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\nplt.figure()\n\nplt.plot(epochs, mae, 'b', label='Training MAE')\nplt.plot(epochs, val_mae, 'r', label='Validation MAE')\nplt.title('Training and Validation MAE')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **LSTM-holdout**","metadata":{}},{"cell_type":"code","source":"train_data = np.reshape(train_data, (train_data.shape[0],train_data.shape[1],1))\nmodel =  keras.Sequential([ \n        tf.keras.layers.LSTM(16, return_sequences = True, activation='relu',input_shape=(train_data.shape[1],1)),\n        tf.keras.layers.LSTM(16, return_sequences = True, activation='relu'),\n        tf.keras.layers.LSTM(16, return_sequences = True, activation='relu'),\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(1)\n    ])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer=tf.keras.optimizers.RMSprop(lr=0.01)\nmodel.compile(optimizer=optimizer, loss='mse', metrics=['mae']) # observe the loss and metrics","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_data = np.reshape(val_data, (val_data.shape[0],val_data.shape[1],1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history=model.fit(train_data, train_labels, epochs=130, batch_size=16,validation_data=(val_data, val_labels),callbacks=[early_stop])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data = np.reshape(test_data, (test_data.shape[0],test_data.shape[1],1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"[mse, mae] = model.evaluate(test_data, test_labels) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ypred = model.predict(test_data)\nprint('Actual Price = {} and Predicted Price = {}'.format(test_labels[2], ypred[2]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let us plot the loss and accuracy curves\nhistory_dict = history.history\nloss_value = history_dict['loss']\nval_loss_value = history_dict['val_loss']\nmae = history_dict['mae']\nval_mae = history_dict['val_mae']\nepochs = range(1, len(loss_value) + 1)\nplt.plot(epochs, loss_value, 'b', label='Training Loss')\nplt.plot(epochs, val_loss_value, 'r', label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\nplt.figure()\n\nplt.plot(epochs, mae, 'b', label='Training MAE')\nplt.plot(epochs, val_mae, 'r', label='Validation MAE')\nplt.title('Training and Validation MAE')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"$$\n\\begin{bmatrix}\n    \\hline\n    \\mathtt{Model} & \\mathtt{MAE-Train} & \\mathtt{MAE-Test} \\\\\n    \\hline \n    DNN-Holdout Validation & 2.1905 & 2.8366 \\\\  \n    DNN-K-fold Validation & 2.6342 & 3.1717 \\\\ \n    DNN with BatchNormalization-Holdout Validation & 3.3871 & 2.701 \\\\\n    GRU-Holdout Validation &  5.74 & 7.06 \\\\\n    Conv1D-Holdout Validation & 6.309 & 6.67 \\\\\n    LSTM-Holdout Validation &  7.24 & 8.99 \\\\ \n    \\hline\n\\end{bmatrix}\n$$","metadata":{}},{"cell_type":"markdown","source":"Hence we conclude that atleast in this case simple DNN is enough no need to go to GRU or LSTM specially due to some signs of overfitting it can be due to the data in this dataset too low for these models to work well. On the other hand DNN- KFold Validation or DNN with Batch Normalization can be also used as it gives results closer to real prices without overfitting. Lasltly although Conv1D doesn't overfit or underfit but result score is not enough so using simple case of DNN with Holdout Validation should work here.","metadata":{}}]}